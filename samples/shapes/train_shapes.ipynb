{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /userhome/comp3314/c3314g15/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /userhome/comp3314/c3314g15/code/Mask_RCNN/logs/shapes20190329T0738/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From /userhome/comp3314/c3314g15/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/comp3314/c3314g15/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/userhome/comp3314/c3314g15/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  2/100 [..............................] - ETA: 44:02 - loss: 6.8781 - rpn_class_loss: 0.0858 - rpn_bbox_loss: 1.9851 - mrcnn_class_loss: 2.7460 - mrcnn_bbox_loss: 1.2003 - mrcnn_mask_loss: 0.8610"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 2\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
